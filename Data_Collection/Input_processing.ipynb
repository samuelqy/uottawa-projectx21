{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\73183\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = open('StopWords.txt') \n",
    "#sw = file.read().splitlines()\n",
    "\n",
    "def remove_punc(line):\n",
    "\n",
    "    return line.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_url(line):\n",
    "\n",
    "    return re.sub(r'http\\S+', '', line)\n",
    "\n",
    "def remove_digits(line):\n",
    "\n",
    "    return re.sub(r'[0-9]+', '', line)\n",
    "\n",
    "def remove_mention(line):\n",
    "    \n",
    "    return re.sub(r'@\\S+', '', line)\n",
    "\n",
    "def cut_sentence(line):\n",
    "    '''\n",
    "    cut the sentence to only keep words before \"depression\"\n",
    "    '''\n",
    "    words = word_tokenize(line)\n",
    "    toRet = [] \n",
    "    for word in words:\n",
    "        toRet.append(word)\n",
    "        if word in [\"depression\", \"depressed\"]:\n",
    "            break\n",
    "    return toRet\n",
    "    \n",
    "def token(line):\n",
    "    '''\n",
    "    tokenize the whole tweet. Returns [[Pos tags sentence 1][Pos tags sentence 2]]\n",
    "    This function only keep sentences contains target word i.e. \"depression\", \"depreseed\".\n",
    "    Only words before the target word will be tokenized.\n",
    "    Each token's POS tag will also be stored\n",
    "    \n",
    "    note: POS tagging is case sensitive, i.e. \"i\" is NN(norn) and \"I\" is PRP(pronoun)\n",
    "    \n",
    "    can add extra procedure to check the case of word and correct misspelling\n",
    "    '''\n",
    "    \n",
    "    #preprocessing\n",
    "    #line = line.lower()\n",
    "    line = remove_mention(line)\n",
    "    line = remove_url(line)\n",
    "    line = remove_digits(line) \n",
    "    line = re.sub(r'â€™', \"'\", line) #some tweets misspelled it and casue problem in nltk\n",
    "    #line = remove_punc(line)\n",
    "    \n",
    "    sentences = sent_tokenize(line)\n",
    "    sentence_candi = list()\n",
    "    target = [\"depression\", \"depressed\"]\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if any(word in sentence for word in target):\n",
    "            sentence_candi.append(nltk.pos_tag(cut_sentence(sentence)))\n",
    "    \n",
    "    return sentence_candi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('I', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('not', 'RB'),\n",
       "  ('handle', 'VB'),\n",
       "  ('my', 'PRP$'),\n",
       "  ('depression', 'NN')]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token(\"im activating that day. I cannot handle my depression and jealousy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def match_depress(tweet):\n",
    "    '''\n",
    "    Return the simplified version of tweet.\n",
    "    \"Simplified\" means: \n",
    "    1. only care about sentence contains target word i.e. depression, depressed\n",
    "    2. remove all word after the target word\n",
    "    3. remove all word in discard_tags, currently contains Determiner(DT), Adjective(JJ, JJR, JJS), Coordinating Conjunction(CC), and Adverb(RB)* \n",
    "    \n",
    "    \n",
    "    note: 1.Aeed optimize tags in discard_tags. Norm(NN) might be a good idea\n",
    "          2.A maximum iteration is implemented here to limit words extracted and make the simplied sentence recognized by my simple CFG tree(defined in next cell)\n",
    "          Maximum iteration can be removed when we have a satisfying CFG tree which can proceed more general sentence\n",
    "          3.The same as break when encounter pronoun. This can be remover when we have a satisfying CFG tree\n",
    "    \n",
    "    *All adverb is removed except \"not\" \"n't\", these words are used to detect negation\n",
    "    '''\n",
    "    res = list()\n",
    "    discard_tags = [\"DT\", \"JJ\", \"JJR\", \"JJS\", \"CC\", \"RB\"] #words with these tags will be discarded\n",
    "    sentences_tags = token(tweet) #sentences in tweet cut at keyword\n",
    "    \n",
    "    #for each cutted sentence\n",
    "    for sentence in sentences_tags:\n",
    "        \n",
    "        sentence = sentence[::-1] #reverse the setence to iterate from keyword \"depression\"\n",
    "        print(sentence)\n",
    "        iteration = 0\n",
    "        \n",
    "        word_buffer = \"\"\n",
    "        \n",
    "        for word in sentence:\n",
    "            \n",
    "            #if (iteration == 0):\n",
    "                #word_buffer = word[0]+\" \" + word_buffer\n",
    "            \n",
    "            #case when reached maximum iteration \n",
    "            if (iteration == 5):\n",
    "                break\n",
    "            \n",
    "            if (word[1] not in discard_tags ) or (word[0] in [\"not\", \"n't\"]):\n",
    "                word_buffer = word[0]+\" \" + word_buffer\n",
    "                if (word[1] in [\"PRP$\", \"PRP\"]): #when encountered pronoun, we have enough information and no need to iterate more i.e. \"my depression\", \"his depression\"\n",
    "                    break\n",
    "            \n",
    "            iteration+=1\n",
    "            print(word_buffer)\n",
    "            \n",
    "            \n",
    "        res.append(word_buffer)\n",
    "        \n",
    "    return res\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('depression', 'NN'), ('my', 'PRP$'), ('handle', 'VB'), ('not', 'RB'), ('can', 'MD'), ('I', 'PRP')]\n",
      "depression \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['my depression ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example to stop at pronoun\n",
    "match_depress(\"im activating that day. I cannot handle my depression and jealousy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here is a simple Context Free Grammar Tree to proceed simplified sentence returned by match_depress().\n",
    "Once the simplified sentence can sucessfully pass this CFG tree (returns sentece structure) it is related to depression.\n",
    "\n",
    "Only words in the ending node(leaf) can be recognized, all other words will triger ValueError. That's why we need to simplify sentence first.\n",
    "Here the CFG tree is a simple one and complex sentences need to be simplified very much to be recognized (see note2,3 in function match_depress ). \n",
    "This might lost useful information \n",
    "\n",
    "Possible optimization in:\n",
    "1. enriching the word in ending node(leaf)\n",
    "2. implementing multiple CFG tree accroding to multiple sentence structure to fit gerenral sentence. \n",
    "   Once a sentence is accpeted by one of them, it is related to depression\n",
    "\n",
    "'''\n",
    "grammar2 = nltk.CFG.fromstring(\"\"\"\n",
    "  S  -> NP VP| NP\n",
    "  NP -> PPRP N | N | PRP\n",
    "  Nom -> Adj Nom | N\n",
    "  VP -> V Adj | V NP | V PP | V VP | V TO VP\n",
    "  PP -> P NP| P PP\n",
    "  PRP -> 'i' \n",
    "  PPRP -> 'my' | 'it'\n",
    "  N -> 'depression' | 'nightmare'\n",
    "  Adj  -> 'depressed' | 'depressing'\n",
    "  V ->  \"'s\"|\"'m\" | 'is' | 'was'  | 'have' | 'detected' | 'diagnosed' | 'feel' | 'feeling' | 'hate' | 'want' | 'go' | 'fail' | 'fell' | 'back' | 'do' | 'did' | 'been'| 'can' |handle\n",
    "  P -> 'on' | 'with' | 'as' | 'into' | 'like' | 'to' | 'since'\n",
    "  TO -> 'to'\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (PRP i)) (VP (V have) (NP (N depression))))\n"
     ]
    }
   ],
   "source": [
    "#sucessful example\n",
    "sent = \"I have depression \"\n",
    "sent = word_tokenize(sent.lower())\n",
    "rd_parser = nltk.ChartParser(grammar2)\n",
    "for tree in rd_parser.parse(sent):\n",
    "      print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: '\"n\\'t\"'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-0f826902fe96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrd_parser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChartParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrd_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m       \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\chart.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, tokens, tree_class)\u001b[0m\n\u001b[0;32m   1482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1483\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1484\u001b[1;33m         \u001b[0mchart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchart_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1485\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtree_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\chart.py\u001b[0m in \u001b[0;36mchart_parse\u001b[1;34m(self, tokens, trace)\u001b[0m\n\u001b[0;32m   1440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1442\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1443\u001b[0m         \u001b[0mchart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_chart_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m         \u001b[0mgrammar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    675\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\", \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    678\u001b[0m                 \u001b[1;34m\"Grammar does not cover some of the \"\u001b[0m \u001b[1;34m\"input words: %r.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: Grammar does not cover some of the input words: '\"n\\'t\"'."
     ]
    }
   ],
   "source": [
    "#failed example (contain word \"n't\" which is not in CFG tree) triger ValueError\n",
    "sent = \"I don't have depression \"\n",
    "sent = word_tokenize(sent.lower())\n",
    "rd_parser = nltk.ChartParser(grammar2)\n",
    "for tree in rd_parser.parse(sent):\n",
    "      print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#failed example 2 (not a sentence structure ) return nothing\n",
    "sent = \"I depression \"\n",
    "sent = word_tokenize(sent.lower())\n",
    "rd_parser = nltk.ChartParser(grammar2)\n",
    "for tree in rd_parser.parse(sent):\n",
    "      print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\73183\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.787, 'neu': 0.213, 'pos': 0.0, 'compound': -0.5719}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"My depression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.767, 'neu': 0.233, 'pos': 0.0, 'compound': -0.5106}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\"feel depressed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.154, 'neu': 0.498, 'pos': 0.349, 'compound': 0.5719}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\"hopefully, depression is better understood and easier to talk about now as compared to the 90's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.406, 'neu': 0.594, 'pos': 0.0, 'compound': -0.8834}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\"Fighting Depression Was My Biggest Flex Last Year Weary face Do Yall Know How Hard It Is To Fight Everyday With Your Mind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.592, 'neu': 0.408, 'pos': 0.0, 'compound': -0.7003}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\"Battling depression nobody knows how I feel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.576, 'neu': 0.424, 'pos': 0.0, 'compound': -0.7003}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\"Me when my anxiety and depression hit hard.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.381, 'neu': 0.619, 'pos': 0.0, 'compound': -0.5719}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\"a 2022 sangi vlive could cure my depression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.346, 'neu': 0.654, 'pos': 0.0, 'compound': -0.5719}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\"Babe wake up, it's your daily itufashi depression\") #this will make sense if the subject of depression is \"my\" not \"your\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.373, 'neu': 0.432, 'pos': 0.195, 'compound': -0.5106}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\"After the supreme being,another thing that I fear in life is depression\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.351, 'neu': 0.649, 'pos': 0.0, 'compound': -0.6597}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\"When anxiety & depression rule, they can lead you to live a smaller life\") #not a good example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea is to first filter tweet by keywords: \"depression\", \"depressed\". Then only cut and conduct dependency analysis on sentence contains keywords. After analyzing the dependency relation of keyword. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en-core-web-sm==3.2.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl#egg=en_core_web_sm==3.2.0 in c:\\users\\73183\\anaconda3\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\73183\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (50.3.1.post20201107)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.24.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.50.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (20.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.8.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2020.6.20)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\73183\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\73183\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.4.3)\n",
      "âœ” Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "!spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spacy.load('en_core_web_sm')\n",
    "tweet = \"I was diagonized with depression\"\n",
    "content = sp(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"b6a9d695a6514773aa8ea9e3390c613e-0\" class=\"displacy\" width=\"650\" height=\"257.0\" direction=\"ltr\" style=\"max-width: none; height: 257.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">was</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">diagonized</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">with</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">depression</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b6a9d695a6514773aa8ea9e3390c613e-0-0\" stroke-width=\"2px\" d=\"M70,122.0 C70,2.0 290.0,2.0 290.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b6a9d695a6514773aa8ea9e3390c613e-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,124.0 L62,112.0 78,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b6a9d695a6514773aa8ea9e3390c613e-0-1\" stroke-width=\"2px\" d=\"M190,122.0 C190,62.0 285.0,62.0 285.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b6a9d695a6514773aa8ea9e3390c613e-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M190,124.0 L182,112.0 198,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b6a9d695a6514773aa8ea9e3390c613e-0-2\" stroke-width=\"2px\" d=\"M310,122.0 C310,62.0 405.0,62.0 405.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b6a9d695a6514773aa8ea9e3390c613e-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M405.0,124.0 L413.0,112.0 397.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b6a9d695a6514773aa8ea9e3390c613e-0-3\" stroke-width=\"2px\" d=\"M430,122.0 C430,62.0 525.0,62.0 525.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b6a9d695a6514773aa8ea9e3390c613e-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M525.0,124.0 L533.0,112.0 517.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(content, style='dep', jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                Dependency           Head                \n",
      "Depression           nsubj                is                  \n",
      "is                   ROOT                 is                  \n",
      "popular              acomp                is                  \n",
      "among                prep                 is                  \n",
      "teenagers            pobj                 among               \n"
     ]
    }
   ],
   "source": [
    "print('{:<20} {:<20} {:<20}'.format('Token', 'Dependency', 'Head'))\n",
    "for token in content:\n",
    "    print('{:<20} {:<20} {:<20}'.format(str(token.text), token.dep_, token.head.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First get the head of keywords:\n",
    "(1) Head of key word is AUX (auxiliary) i.e. i'm depressed. He is depressed. Find the subject of AUX. Pass if the subject is first person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
