{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [WinError 10054] An existing connection was forcibly\n",
      "[nltk_data]     closed by the remote host>\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = open('StopWords.txt') \n",
    "#sw = file.read().splitlines()\n",
    "\n",
    "def remove_punc(line):\n",
    "\n",
    "    return line.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_url(line):\n",
    "\n",
    "    return re.sub(r'http\\S+', '', line)\n",
    "\n",
    "def remove_digits(line):\n",
    "\n",
    "    return re.sub(r'[0-9]+', '', line)\n",
    "\n",
    "def remove_mention(line):\n",
    "    \n",
    "    return re.sub(r'@\\S+', '', line)\n",
    "\n",
    "def cut_sentence(line):\n",
    "    '''\n",
    "    cut the sentence to only keep words before \"depression\"\n",
    "    '''\n",
    "    words = word_tokenize(line)\n",
    "    toRet = [] \n",
    "    for word in words:\n",
    "        toRet.append(word)\n",
    "        if word in [\"depression\", \"depressed\"]:\n",
    "            break\n",
    "    return toRet\n",
    "    \n",
    "def token(line):\n",
    "    '''\n",
    "    tokenize the whole tweet. \n",
    "    This function only keep sentences contains target word i.e. \"depression\", \"depreseed\".\n",
    "    Only words before the target word will be tokenized.\n",
    "    Each token's POS tag will also be stored\n",
    "    \n",
    "    note: POS tagging is case sensitive, i.e. \"i\" is NN(norn) and \"I\" is PRP(pronoun)\n",
    "    \n",
    "    can add extra procedure to check the case of word and correct misspelling\n",
    "    '''\n",
    "    \n",
    "    #preprocessing\n",
    "    #line = line.lower()\n",
    "    line = remove_mention(line)\n",
    "    line = remove_url(line)\n",
    "    line = remove_digits(line) \n",
    "    line = re.sub(r'â€™', \"'\", line)\n",
    "    #line = remove_punc(line)\n",
    "    \n",
    "    sentences = sent_tokenize(line)\n",
    "    sentence_candi = list()\n",
    "    target = [\"depression\", \"depressed\"]\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if any(word in sentence for word in target):\n",
    "            sentence_candi.append(nltk.pos_tag(cut_sentence(sentence)))\n",
    "    \n",
    "    return sentence_candi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('I', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('not', 'RB'),\n",
       "  ('handle', 'VB'),\n",
       "  ('my', 'PRP$'),\n",
       "  ('depression', 'NN')]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token(\"im activating that day. I cannot handle my depression and jealousy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def match_depress(tweet):\n",
    "    '''\n",
    "    Return the simplified version of tweet.\n",
    "    \"Simplified\" means: \n",
    "    1. only care about sentence contains target word i.e. depression, depressed\n",
    "    2. remove all word after the target word\n",
    "    3. remove all word in discard_tags, currently contains Determiner(DT), Adjective(JJ, JJR, JJS), Coordinating Conjunction(CC), and Adverb(RB)* \n",
    "    \n",
    "    \n",
    "    note: 1.Aeed optimize tags in discard_tags. Norm(NN) might be a good idea\n",
    "          2.A maximum iteration is implemented here to limit words extracted and make the simplied sentence recognized by my simple CFG tree(defined in next cell)\n",
    "          Maximum iteration can be removed when we have a satisfying CFG tree which can proceed more general sentence\n",
    "          3.The same as break when encounter pronoun. This can be remover when we have a satisfying CFG tree\n",
    "    \n",
    "    *All adverb is removed except \"not\" \"n't\", these words are used to detect negation\n",
    "    '''\n",
    "    res = list()\n",
    "    discard_tags = [\"DT\", \"JJ\", \"JJR\", \"JJS\", \"CC\", \"RB\"] #words with these tags will be discarded\n",
    "    sentences_tags = token(tweet) #sentences in tweet cut at keyword\n",
    "    \n",
    "    #for each cutted sentence\n",
    "    for sentence in sentences_tags:\n",
    "        \n",
    "        sentence = sentence[::-1] #reverse the setence to iterate from keyword \"depression\"\n",
    "        print(sentence)\n",
    "        iteration = 0\n",
    "        \n",
    "        word_buffer = \"\"\n",
    "        \n",
    "        for word in sentence:\n",
    "            \n",
    "            #if (iteration == 0):\n",
    "                #word_buffer = word[0]+\" \" + word_buffer\n",
    "            \n",
    "            #case when reached maximum iteration \n",
    "            if (iteration == 5):\n",
    "                break\n",
    "            \n",
    "            if (word[1] not in discard_tags ) or (word[0] in [\"not\", \"n't\"]):\n",
    "                word_buffer = word[0]+\" \" + word_buffer\n",
    "                if (word[1] in [\"PRP$\", \"PRP\"]): #when encountered pronoun, we have enough information and no need to iterate more i.e. \"my depression\", \"his depression\"\n",
    "                    break\n",
    "            \n",
    "            iteration+=1\n",
    "            print(word_buffer)\n",
    "            \n",
    "            \n",
    "        res.append(word_buffer)\n",
    "        \n",
    "    return res\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('depression', 'NN'), ('my', 'PRP$'), ('handle', 'VB'), ('not', 'RB'), ('can', 'MD'), ('I', 'PRP')]\n",
      "depression \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['my depression ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example to stop at pronoun\n",
    "match_depress(\"im activating that day. I cannot handle my depression and jealousy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here is a simple Context Free Grammar Tree to proceed simplified sentence returned by match_depress().\n",
    "Once the simplified sentence can sucessfully pass this CFG tree (returns sentece structure) it is related to depression.\n",
    "\n",
    "Only words in the ending node(leaf) can be recognized, all other words will triger ValueError. That's why we need to simplify sentence first.\n",
    "Here the CFG tree is a simple one and complex sentences need to be simplified very much to be recognized (see note2,3 in function match_depress ). \n",
    "This might lost useful information \n",
    "\n",
    "Possible optimization in:\n",
    "1. enriching the word in ending node(leaf)\n",
    "2. implementing multiple CFG tree accroding to multiple sentence structure to fit gerenral sentence. \n",
    "   Once a sentence is accpeted by one of them, it is related to depression\n",
    "\n",
    "'''\n",
    "grammar2 = nltk.CFG.fromstring(\"\"\"\n",
    "  S  -> NP VP| NP\n",
    "  NP -> PPRP N | N | PRP\n",
    "  Nom -> Adj Nom | N\n",
    "  VP -> V Adj | V NP | V PP | V VP | V TO VP\n",
    "  PP -> P NP| P PP\n",
    "  PRP -> 'i' \n",
    "  PPRP -> 'my' | 'it'\n",
    "  N -> 'depression' | 'nightmare'\n",
    "  Adj  -> 'depressed' | 'depressing'\n",
    "  V ->  \"'s\"|\"'m\" | 'is' | 'was'  | 'have' | 'detected' | 'diagnosed' | 'feel' | 'feeling' | 'hate' | 'want' | 'go' | 'fail' | 'fell' | 'back' | 'do' | 'did' | 'been'| 'can' |handle\n",
    "  P -> 'on' | 'with' | 'as' | 'into' | 'like' | 'to' | 'since'\n",
    "  TO -> 'to'\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (PRP i)) (VP (V have) (NP (N depression))))\n"
     ]
    }
   ],
   "source": [
    "#sucessful example\n",
    "sent = \"I have depression \"\n",
    "sent = word_tokenize(sent.lower())\n",
    "rd_parser = nltk.ChartParser(grammar2)\n",
    "for tree in rd_parser.parse(sent):\n",
    "      print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: '\"n\\'t\"'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-0f826902fe96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrd_parser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChartParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrd_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m       \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\chart.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, tokens, tree_class)\u001b[0m\n\u001b[0;32m   1482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1483\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1484\u001b[1;33m         \u001b[0mchart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchart_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1485\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtree_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\chart.py\u001b[0m in \u001b[0;36mchart_parse\u001b[1;34m(self, tokens, trace)\u001b[0m\n\u001b[0;32m   1440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1442\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1443\u001b[0m         \u001b[0mchart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_chart_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m         \u001b[0mgrammar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    675\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\", \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    678\u001b[0m                 \u001b[1;34m\"Grammar does not cover some of the \"\u001b[0m \u001b[1;34m\"input words: %r.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: Grammar does not cover some of the input words: '\"n\\'t\"'."
     ]
    }
   ],
   "source": [
    "#failed example (contain word \"n't\" which is not in CFG tree) triger ValueError\n",
    "sent = \"I don't have depression \"\n",
    "sent = word_tokenize(sent.lower())\n",
    "rd_parser = nltk.ChartParser(grammar2)\n",
    "for tree in rd_parser.parse(sent):\n",
    "      print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#failed example 2 (not a sentence structure ) return nothing\n",
    "sent = \"I depression \"\n",
    "sent = word_tokenize(sent.lower())\n",
    "rd_parser = nltk.ChartParser(grammar2)\n",
    "for tree in rd_parser.parse(sent):\n",
    "      print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
