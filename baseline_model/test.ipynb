{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from progressbar import ProgressBar\n",
    "from PIL import ImageFile\n",
    "from torchvision.models import vgg16\n",
    "import torch\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from multiprocessing import Pool\n",
    "from string import punctuation\n",
    "import pickle\n",
    "import shutil\n",
    "from PIL import UnidentifiedImageError\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import torch.cuda\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "from torchvision import models, transforms\n",
    "model = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureExtractor(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (pooling): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc): Linear(in_features=25088, out_features=4096, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "  def __init__(self, model):\n",
    "    super(FeatureExtractor, self).__init__()\n",
    "\t\t# Extract VGG-16 Feature Layers\n",
    "    self.features = list(model.features)\n",
    "    self.features = nn.Sequential(*self.features)\n",
    "\t\t# Extract VGG-16 Average Pooling Layer\n",
    "    self.pooling = model.avgpool\n",
    "\t\t# Convert the image into one-dimensional vector\n",
    "    self.flatten = nn.Flatten()\n",
    "\t\t# Extract the first part of fully-connected layer from VGG16\n",
    "    self.fc = model.classifier[0]\n",
    "  \n",
    "  def forward(self, x):\n",
    "\t\t# It will take the input 'x' until it returns the feature vector called 'out'\n",
    "    out = self.features(x)\n",
    "    out = self.pooling(out)\n",
    "    out = self.flatten(out)\n",
    "    #out = self.fc(out) \n",
    "    return out \n",
    "\n",
    "# Initialize the model\n",
    "model = models.vgg16(pretrained=True)\n",
    "new_model = FeatureExtractor(model)\n",
    "\n",
    "# Change the device to GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "new_model = new_model.to(device)\n",
    "new_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(open(r'C:\\Users\\73183\\Documents\\ProjectX\\DepressionMultiModal\\baseline_model\\new_ds\\negative\\22032403\\1458489236924010498.jpg', 'rb'))\n",
    "image = image.convert('RGB')  # 将所有图片转换成3通道\n",
    "x, y = image.size\n",
    "if x > y:\n",
    "    x_ = 224 * x // y\n",
    "    y_ = 224\n",
    "else:\n",
    "    x_ = 224\n",
    "    y_ = 224 * y // x\n",
    "image = image.resize((x_, y_), Image.ANTIALIAS)\n",
    "image = transform(image).unsqueeze(0).to(device)\n",
    "print(image.size())\n",
    "#image = image[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\t\t# Extract the feature from the image\n",
    "    feature = new_model(image)\n",
    "\t# Convert to NumPy Array, Reshape it, and save it to features variable\n",
    "\n",
    "feature.cpu().detach().numpy().reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\73183\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def cut_sentence(line):\n",
    "\n",
    "    words = word_tokenize(line)\n",
    "    toRet = [] \n",
    "    for word in words:\n",
    "        toRet.append(word)\n",
    "        if word in [\"depression\", \"depressed\"]:\n",
    "            break\n",
    "    return toRet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "20\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "counter = 0\n",
    "current = os.getcwd()\n",
    "\n",
    "for user in neg_users:\n",
    "    text_dir = os.path.join(current,'new_ds', 'negative', user, 'timeline.txt')\n",
    "    user_counter = 0\n",
    "    with open(text_dir) as f:\n",
    "\n",
    "        tweets = f.readlines()\n",
    "        \n",
    "        for tweet in tweets:\n",
    "\n",
    "            j_tweet = json.loads(tweet)\n",
    "            text = j_tweet['full_text']\n",
    "\n",
    "            if \"depress\" in text:\n",
    "                list = cut_sentence(text)\n",
    "                sentence = \" \".join(list)\n",
    "                user_counter+= 1\n",
    "    if user_counter != 0:\n",
    "        print(user_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('final.txt', 'w') as f:\n",
    "    # print(f.name, image_path[:-4]+'.pkl')\n",
    "    f.write(str(feature.tolist()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cc958fe196fd4bc290dcc87cf2b67961cf589198de9a84d21e798d0d745b9d1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
